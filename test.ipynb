{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lesong/anaconda3/envs/simpo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import json\n",
    "\n",
    "all_data = []\n",
    "# ds_DBAR = load_from_disk(\"/home/lesong/codes/SimPO/datasets/llama3.1_8B_ultrafeedback/cpo_dataset_1\")\n",
    "with open(\"/home/lesong/codes/SimPO/on_policy_data_gen/datasets/llama3.1_8B_ultrafeedback/all_outputs_rm.json\", \"r\") as f:\n",
    "    ds_DBAR = json.load(f)\n",
    "\n",
    "for item in ds_DBAR:\n",
    "    all_data.append({\n",
    "        \"prompt\": item[\"prompt\"],\n",
    "        \"chosen\": item[\"chosen\"],\n",
    "        \"rejected\": item[\"rejected\"],\n",
    "        \"type\": \"DBAR\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "all_data = []\n",
    "\n",
    "with jsonlines.open(\"/home/lesong/codes/SimPO/data/cpo_data.json\") as reader:\n",
    "    for item in reader:\n",
    "        all_data.append({\n",
    "            \"prompt\": item[\"prompt\"],\n",
    "            \"chosen\": item[\"chosen\"],\n",
    "            \"rejected\": item[\"rejected\"],\n",
    "            \"type\": \"D\"\n",
    "    })\n",
    "\n",
    "with jsonlines.open(\"/home/lesong/codes/SimPO/data/all_train_data.json\") as reader:\n",
    "    for item in reader:\n",
    "        if item[\"prompt\"].startswith(\"Your task is to decide whether question decomposition is necessary to answer a given question\"): continue\n",
    "        all_data.append({\n",
    "        \"prompt\": item[\"prompt\"],\n",
    "        \"chosen\": item[\"chosen\"],\n",
    "        \"rejected\": item[\"rejected\"],\n",
    "        \"type\": \"DBAR\"\n",
    "    })\n",
    "\n",
    "        \n",
    "with jsonlines.open(\"data/all_train_data.json\", \"w\") as writter:\n",
    "    writter.write_all(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Your task is to decide whether question decomposition is necessary to answer a given question, if yes, which sub-question shall be asked. \\n    Do not ask same or similar sub-questions that you have asked before.\\n    Decompose the question in a reasonable and smart way. \\n    Only give one sub-question each time.\\n    Sub questions that you have asked before, if any:\\n                \\n                \\n    Think step by step.\\n    Based on the given context, provide your answer to the following question: What is the river where Long Spruce Generating Station is located a tributary of?\\n    Your answer shall be in the following JSON format:\\n    {\\n        \"decomposition\": true, \\n        \"sub question\": <A sub question that you would like to ask>\\n    }\\n    OR\\n    {\"decomposition\": false}\\n    ', 'chosen': [{'content': 'Your task is to decide whether question decomposition is necessary to answer a given question, if yes, which sub-question shall be asked. \\n    Do not ask same or similar sub-questions that you have asked before.\\n    Decompose the question in a reasonable and smart way. \\n    Only give one sub-question each time.\\n    Sub questions that you have asked before, if any:\\n                \\n                \\n    Think step by step.\\n    Based on the given context, provide your answer to the following question: What is the river where Long Spruce Generating Station is located a tributary of?\\n    Your answer shall be in the following JSON format:\\n    {\\n        \"decomposition\": true, \\n        \"sub question\": <A sub question that you would like to ask>\\n    }\\n    OR\\n    {\"decomposition\": false}\\n    ', 'role': 'user'}, {'content': '{\"decomposition\": true, \"sub question\": \" On which river is the Long Spruce Generating Station located?\"}', 'role': 'assistant'}], 'rejected': [{'content': 'Your task is to decide whether question decomposition is necessary to answer a given question, if yes, which sub-question shall be asked. \\n    Do not ask same or similar sub-questions that you have asked before.\\n    Decompose the question in a reasonable and smart way. \\n    Only give one sub-question each time.\\n    Sub questions that you have asked before, if any:\\n                \\n                \\n    Think step by step.\\n    Based on the given context, provide your answer to the following question: What is the river where Long Spruce Generating Station is located a tributary of?\\n    Your answer shall be in the following JSON format:\\n    {\\n        \"decomposition\": true, \\n        \"sub question\": <A sub question that you would like to ask>\\n    }\\n    OR\\n    {\"decomposition\": false}\\n    ', 'role': 'user'}, {'content': '{\"decomposition\": true, \"sub question\": \" From what does Arthur Curry\\'s metahuman aquatic abilities originate in the \"Justice League\" film?\"}', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "from vllm import LLM, SamplingParams\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "llm = LLM(model=args.model)\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "train_dataset= load_dataset(data_dir, split='train_prefs')\n",
    "\n",
    "prompts = sorted(list(set(train_dataset['prompt'])))\n",
    "\n",
    "conversations = [tokenizer.apply_chat_template([{'role': 'user', 'content': prompt}], tokenize=False, add_generation_prompt=True) for prompt in prompts]\n",
    "\n",
    "sampling_params = SamplingParams(temperature=args.temperature, \n",
    "                                 top_p=args.top_p, \n",
    "                                 max_tokens=args.max_tokens, \n",
    "                                 seed=args.seed,)\n",
    "outputs = llm.generate(conversations, sampling_params)\n",
    "\n",
    "# Save the outputs as a JSON file.\n",
    "output_data = []\n",
    "for i, output in enumerate(outputs):\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    output_data.append({\n",
    "        'prompt': prompts[i],\n",
    "        \"format_prompt\": prompt,\n",
    "        'generated_text': generated_text,\n",
    "    })\n",
    "\n",
    "\n",
    "all_data = []\n",
    "with jsonlines.open(\"./on_policy_data_gen/datasets/llama3.1_8B_ultrafeedback/cpo_data.json\") as reader:\n",
    "    for obj in reader:\n",
    "        obj[\"type\"] = \"D\"\n",
    "        obj[\"rejected\"][1][\"content\"] = \n",
    "        print(obj)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lesong/anaconda3/envs/simpo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_name = \"/home/lesong/codes/SimPO/outputs/llama-3-3b-instruct-simpo-v2_1\"\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "# model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# llm_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "# model.load_adapter(peft_model_id)\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    # \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# llm_model.load_adapter(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def parse_decompose_response(raw_resp):\n",
    "    resp = {\"decomposition\": False}\n",
    "    start_idx, end_idx = raw_resp.find(\"<sub question>\"), raw_resp.find(\"</sub question>\")\n",
    "    if start_idx >= 0 and end_idx >= 0:\n",
    "        sub_question = raw_resp[start_idx+len(\"<sub question>\"):end_idx].strip()\n",
    "        resp[\"sub question\"] = sub_question\n",
    "        start_idx, end_idx = raw_resp.find(\"<decomposition>\"), raw_resp.find(\"</decomposition>\")\n",
    "        if start_idx >= 0 and end_idx >= 0:\n",
    "            decomposition = (True if raw_resp[start_idx+len(\"<decomposition>\"):end_idx].strip().lower() == \"true\" else False)\n",
    "            resp[\"decomposition\"] = decomposition\n",
    "    return resp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected', 'type'],\n",
       "    num_rows: 1482\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"/home/lesong/codes/SimPO/datasets/llama3.1_8B_ultrafeedback/cpo_dataset_1\")\n",
    "dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round:  0\n",
      "responses:  ['<decomposition>true</decomposition>\\n<sub_question>Who was the king of England in 1951?</sub_question>']\n",
      "[]\n",
      "<decomposition>true</decomposition> \n",
      "    <sub question>What is the nationality of Lionel Logue, the speech and language therapist in \"The King's Speech\"?</sub question>\n",
      "round:  1\n",
      "responses:  ['<decomposition>true</decomposition>\\n<sub_question>Who was the king of England in 1951?</sub_question>']\n",
      "[]\n",
      "<decomposition>true</decomposition> \n",
      "    <sub question>What is the nationality of Lionel Logue, the speech and language therapist in \"The King's Speech\"?</sub question>\n",
      "round:  2\n",
      "responses:  ['<decomposition>true</decomposition>\\n<sub_question>Who was the king of England in 1951?</sub_question>']\n",
      "[]\n",
      "<decomposition>true</decomposition> \n",
      "    <sub question>What is the nationality of Lionel Logue, the speech and language therapist in \"The King's Speech\"?</sub question>\n",
      "round:  3\n",
      "responses:  ['<decomposition>true</decomposition>\\n<sub_question>Who was the king of England in 1951?</sub_question>']\n",
      "[]\n",
      "<decomposition>true</decomposition> \n",
      "    <sub question>What is the nationality of Lionel Logue, the speech and language therapist in \"The King's Speech\"?</sub question>\n",
      "round:  4\n",
      "responses:  ['<decomposition>true</decomposition>\\n<sub_question>Who was the king of England in 1951?</sub_question>']\n",
      "[]\n",
      "<decomposition>true</decomposition> \n",
      "    <sub question>What is the nationality of Lionel Logue, the speech and language therapist in \"The King's Speech\"?</sub question>\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "prompt = np.random.choice(dataset[\"test\"])\n",
    "\n",
    "\n",
    "def get_response(prompt, llm_model, tokenizer):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt[\"prompt\"]}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(llm_model.device)\n",
    "    \n",
    "    terminators = [\n",
    "            tokenizer.eos_token_id,\n",
    "            # tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    \n",
    "\n",
    "    generated_ids = llm_model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id = tokenizer.eos_token_id,\n",
    "        temperature=1.2, \n",
    "        top_p=1.2, \n",
    "        top_k=20, \n",
    "        num_beams=5,\n",
    "        do_sample=True\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    print(\"responses: \", responses)\n",
    "    sub_questions = []\n",
    "    for resp in responses:\n",
    "        resp = parse_decompose_response(resp)\n",
    "        if resp[\"decomposition\"]:\n",
    "            sub_questions.append(resp[\"sub question\"])\n",
    "    \n",
    "    return list(set(sub_questions))\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"round: \", i)\n",
    "    print(get_response(prompt, llm_model, tokenizer))\n",
    "    print(prompt[\"chosen\"][1][\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simpo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
